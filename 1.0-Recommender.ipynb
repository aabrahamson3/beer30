{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import custom functions and required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import functions\n",
    "from surprise import SVD, Dataset, accuracy, BaselineOnly, Reader, KNNWithMeans, KNNBasic, NormalPredictor\n",
    "from surprise.model_selection import cross_validate, train_test_split, GridSearchCV\n",
    "from collections import defaultdict\n",
    "from surprise.model_selection import KFold\n",
    "# import pyspark as spark\n",
    "# from pyspark.sql import SparkSession, Row\n",
    "# from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# from pyspark.ml.recommendation import ALS\n",
    "# spark = SparkSession.builder.appName('Recommendation_system').getOrCreate()\n",
    "# from pyspark.ml.feature import StringIndexer\n",
    "# from pyspark.ml import Pipeline\n",
    "# from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "beers = pd.read_csv('beers-breweries-and-beer-reviews/beers.csv')\n",
    "breweries = pd.read_csv('beers-breweries-and-beer-reviews/breweries.csv')\n",
    "reviews = pd.read_csv('beers-breweries-and-beer-reviews/reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess reviews and beer csvs and output a DF ready to be used by collaborative filter\n",
    "df_with_mins = functions.preprocess_reviews(reviews,beers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1948703, 23)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_mins.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This proved to not be a better performer than the surprise SVD, feel free to uncomment the imports in the first cell if you'd like to run through these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = df_with_mins[['username', 'id', 'score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "mySchema = StructType([StructField('username', StringType(), True),\\\n",
    "                      StructField('id', IntegerType(), True),\\\n",
    "                      StructField('score', FloatType(), True)\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = spark.createDataFrame(df_spark, schema=mySchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = [StringIndexer(inputCol=column, outputCol=column+\"_index\")\\\n",
    "          for column in list(set(df_spark.columns)-set(['id','score']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=indexer)\n",
    "transformed = pipeline.fit(df_spark).transform(df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training,test) = transformed.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "als=ALS(maxIter=5,regParam=0.09,rank=25,\\\n",
    "        userCol=\"username_index\",itemCol=\"id\",\\\n",
    "        ratingCol=\"score\",coldStartStrategy=\"drop\",\n",
    "        nonnegative=True)\n",
    "\n",
    "model=als.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator=RegressionEvaluator(metricName=\"rmse\",\n",
    "                              labelCol=\"score\",\n",
    "                              predictionCol=\"prediction\")\n",
    "predictions=model.transform(test)\n",
    "rmse=evaluator.evaluate(predictions)\n",
    "print(\"RMSE=\"+str(rmse))\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_recs=model.recommendForAllUsers(20).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed.filter(transformed.username_index == 1580).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_mins.loc[(df_with_mins.id == 78820) &\n",
    "                 (df_with_mins.username == 'hoppytobehere')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SURPRISE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Surprise SVD proved to be great to work with, however the resulting recommender proved to not adequately recommend a variety of beers/breweries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the rating scale to 1-5, and create the user/item matrix\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(df_with_mins[['username', 'id', 'score']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_rmse': array([0.82007073, 0.82050574]),\n",
       " 'test_mae': array([0.64395869, 0.64446125]),\n",
       " 'fit_time': (1.650062084197998, 1.865022897720337),\n",
       " 'test_time': (14.152489185333252, 12.945470809936523)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross validate data with a normal predictor to get a baseline\n",
    "cross_validate(NormalPredictor(), data, cv=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.4189\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.41885171629375584"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test set is made of 20% of the ratings.\n",
    "trainset, testset = train_test_split(data, test_size=.2)\n",
    "\n",
    "# Using Surprise's SVD model\n",
    "algo = SVD()\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Then compute RMSE\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the below grid search took approx. 35 minutes on my local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search with SVD\n",
    "params = {'n_factors': [20, 50, 100],\n",
    "         'reg_all': [0.01, 0.02, 0.05]}\n",
    "# n_jobs = -1 means all CPUs are used\n",
    "gs_svd = GridSearchCV(SVD, param_grid=params, n_jobs=-1)\n",
    "gs_svd.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_svd.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_svd.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8497791118294428\n",
      "0.6053896940054159\n",
      "0.8446046612265048\n",
      "0.6087237132677664\n",
      "0.8491118222716928\n",
      "0.6063283369361788\n",
      "0.8487067631953434\n",
      "0.6082032354045026\n",
      "0.8480427708042743\n",
      "0.6056180260920596\n"
     ]
    }
   ],
   "source": [
    "# Perform KFold Cross Validation with best params from grid search, then output \n",
    "# evaluation metrics\n",
    "kf = KFold(n_splits=5)\n",
    "algo = SVD(n_factors= 20, reg_all= 0.02)\n",
    "\n",
    "for trainset, testset in kf.split(data):\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    precisions, recalls = functions.precision_recall_at_k(predictions, k=5, threshold=4)\n",
    "\n",
    "    # Precision and recall can then be averaged over all users\n",
    "    print(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "    print(sum(rec for rec in recalls.values()) / len(recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initial precision is ~84% @ k=5 and, 4 as threshold \n",
    "## initial recall is 60.3%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trainset, testset in kf.split(data):\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    precisions, recalls = precision_recall_at_k(predictions, k=3, threshold=4)\n",
    "\n",
    "    # Precision and recall can then be averaged over all users\n",
    "    print(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "    print(sum(rec for rec in recalls.values()) / len(recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trainset, testset in kf.split(data):\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    precisions, recalls = precision_recall_at_k(predictions, k=1, threshold=4.2)\n",
    "\n",
    "    # Precision and recall can then be averaged over all users\n",
    "    print(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "    print(sum(rec for rec in recalls.values()) / len(recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trainset, testset in kf.split(data):\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    precisions, recalls = precision_recall_at_k(predictions, k=2, threshold=4.2)\n",
    "\n",
    "    # Precision and recall can then be averaged over all users\n",
    "    print(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "    print(sum(rec for rec in recalls.values()) / len(recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How is A/P when predicting many beers?\n",
    "for trainset, testset in kf.split(data):\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    precisions, recalls = precision_recall_at_k(predictions, k=200, threshold=4.2)\n",
    "\n",
    "    # Precision and recall can then be averaged over all users\n",
    "    print(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "    print(sum(rec for rec in recalls.values()) / len(recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8411080671117886\n",
      "0.569058884423432\n",
      "0.8407760575004992\n",
      "0.5750244368260659\n",
      "0.8399386974562533\n",
      "0.5742876049736397\n",
      "0.8424135710252793\n",
      "0.5751437131537722\n",
      "0.8444531962150195\n",
      "0.574099796311424\n"
     ]
    }
   ],
   "source": [
    "# How is A/P when predicting many beers?\n",
    "for trainset, testset in kf.split(data):\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    precisions, recalls = functions.precision_recall_at_k(predictions, k=400, threshold=4.2)\n",
    "\n",
    "    # Precision and recall can then be averaged over all users\n",
    "    print(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "    print(sum(rec for rec in recalls.values()) / len(recalls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at all users, instead of the 2STD subgroup. Not expecting it to improve scores but thought I should check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the below did not have a significant effect on recall or precision\n",
    "\n",
    "# reader = Reader(rating_scale=(1, 5))\n",
    "# data2 = Dataset.load_from_df(df_all_users[['username', 'id', 'score']], reader)\n",
    "# trainset, testset = train_test_split(data2, test_size=.2)\n",
    "\n",
    "# # Using Surprise's SVD model\n",
    "# algo = SVD()\n",
    "\n",
    "# # Train the algorithm on the trainset, and predict ratings for the testset\n",
    "# algo.fit(trainset)\n",
    "# predictions = algo.test(testset)\n",
    "\n",
    "# # Then compute RMSE\n",
    "# accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this hurts the computer\n",
    "knn_means = KNNWithMeans(k=10, sim_options={'name':'pearson', 'user_based':True})\n",
    "cv_knn_means = cross_validate(knn_means, data, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are other models too: https://surprise.readthedocs.io/en/stable/prediction_algorithms_package.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's make some functions to spit out location based results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_df = df_with_mins[['username', 'id', 'score']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x159ab7400>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate SVD algorithm, train/test split and fit train data\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(df_with_mins[['username', 'id', 'score']], reader)\n",
    "trainset, testset = train_test_split(data, test_size=.2)\n",
    "algo = SVD(n_factors= 20, reg_all= 0.02)\n",
    "algo.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing a prediction for an existing user\n",
    "uid = str('GratefulBeerGuy')\n",
    "iid = 265678"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: GratefulBeerGuy item: 265678     r_ui = None   est = 4.05   {'was_impossible': False}\n"
     ]
    }
   ],
   "source": [
    "pred = algo.predict(uid, iid, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GratefulBeerGuy</td>\n",
       "      <td>125646</td>\n",
       "      <td>4.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GratefulBeerGuy</td>\n",
       "      <td>47678</td>\n",
       "      <td>3.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GratefulBeerGuy</td>\n",
       "      <td>71930</td>\n",
       "      <td>4.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GratefulBeerGuy</td>\n",
       "      <td>326798</td>\n",
       "      <td>3.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GratefulBeerGuy</td>\n",
       "      <td>48824</td>\n",
       "      <td>4.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          username      id  score\n",
       "0  GratefulBeerGuy  125646   4.58\n",
       "1  GratefulBeerGuy   47678   3.69\n",
       "2  GratefulBeerGuy   71930   4.37\n",
       "3  GratefulBeerGuy  326798   3.99\n",
       "4  GratefulBeerGuy   48824   4.51"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to inpute new ratings\n",
    "user_rating = [{'username': 'tester-mctestyface', 'id': 55245, 'score': 5},\n",
    "               {'username': 'tester-mctestyface', 'id': 237806, 'score': 3},\n",
    "               {'username': 'tester-mctestyface', 'id': 1062, 'score': 4},\n",
    "               {'username': 'tester-mctestyface', 'id': 9353, 'score': 5},\n",
    "               {'username': 'tester-mctestyface', 'id': 9353, 'score': 2},\n",
    "               {'username': 'tester-mctestyface', 'id': 1286, 'score': 1.5},\n",
    "              ]\n",
    "\n",
    "## add the new ratings to the original ratings DataFrame\n",
    "new_ratings_df = svd_df.append(user_rating,ignore_index=True)\n",
    "new_data = Dataset.load_from_df(new_ratings_df,reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = algo.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(uid='comfortablynumb1', iid=1901, r_ui=3.58, est=3.8079624162638273, details={'was_impossible': False})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this makes a dict for all users and ranks their predictions. I believe it is only on\n",
    "# test data\n",
    "\n",
    "user_est_true = defaultdict(list)\n",
    "for uid, iid, true_r, est, _ in predictions:\n",
    "    user_est_true[uid].append((est, iid, true_r))\n",
    "\n",
    "\n",
    "for uid, user_ratings in user_est_true.items():\n",
    "\n",
    "    # Sort user ratings by estimated value\n",
    "    user_ratings.sort(key=lambda x: x[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this predicts ratings for all beers for a single user, these are random users to test\n",
    "# predictions\n",
    "egads = functions.user_recs(algo, df_with_mins, 'EgadBananas')\n",
    "macca = functions.user_recs(algo, df_with_mins, 'Macca')\n",
    "gbg = functions.user_recs(algo, df_with_mins, 'GratefulBeerGuy')\n",
    "orioles = functions.user_recs(algo, df_with_mins, 'oriolesfan4')\n",
    "w = functions.user_recs(algo, df_with_mins, 'CJDUBYA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "beers_lookup = beers[['id','brewery_id', 'name']]\n",
    "beers_lookup = beers_lookup.rename(columns={'id':'beer_id'})\n",
    "breweries = breweries.rename(columns={'id':'brewery_id', 'name':'brewery_name'})\n",
    "breweries_lookup = breweries[['brewery_id', 'city', 'state', 'country', 'brewery_name']]\n",
    "beer_breweries_lookup = pd.merge(beers_lookup, breweries_lookup, on='brewery_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "beers_dict = beers_lookup.set_index('beer_id').to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict with beer_id as key, and the value is a dict with brewery_id, city, state, country \n",
    "# as the keys to that dictionary\n",
    "beer_breweries_lookup = beer_breweries_lookup.set_index('beer_id').to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Fremont Brewing Company', 149554, 'Coffee Cinnamon B-Bomb')\n",
      "('Holy Mountain Brewing Company', 216398, 'Midnight Still')\n",
      "('Elysian Brewing Company', 192252, 'The Fix')\n",
      "(\"Reuben's Brews\", 113560, \"Blimey That's Bitter!\")\n"
     ]
    }
   ],
   "source": [
    "functions.output_brewery(functions.location_filter(gbg, beer_breweries_lookup,\\\n",
    "                                                   'WA','Seattle', 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Fremont Brewing Company', 116702, 'The Rusty Nail')\n",
      "('Holy Mountain Brewing Company', 216398, 'Midnight Still')\n",
      "('Elysian Brewing Company', 192252, 'The Fix')\n",
      "(\"Reuben's Brews\", 113560, \"Blimey That's Bitter!\")\n"
     ]
    }
   ],
   "source": [
    "functions.output_brewery(functions.location_filter(macca, beer_breweries_lookup,\\\n",
    "                                                   'WA','Seattle', 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Fremont Brewing Company', 149554, 'Coffee Cinnamon B-Bomb')\n",
      "('Holy Mountain Brewing Company', 216398, 'Midnight Still')\n",
      "('Elysian Brewing Company', 33394, 'The Great Pumpkin')\n",
      "(\"Reuben's Brews\", 113560, \"Blimey That's Bitter!\")\n"
     ]
    }
   ],
   "source": [
    "functions.output_brewery(functions.location_filter(egads, beer_breweries_lookup,\\\n",
    "                                                 'WA','Seattle', 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Fremont Brewing Company', 116702, 'The Rusty Nail')\n",
      "('Holy Mountain Brewing Company', 216398, 'Midnight Still')\n",
      "('Elysian Brewing Company', 192252, 'The Fix')\n",
      "(\"Reuben's Brews\", 113560, \"Blimey That's Bitter!\")\n"
     ]
    }
   ],
   "source": [
    "functions.output_brewery(functions.location_filter(orioles, beer_breweries_lookup,\\\n",
    "                                                   'WA','Seattle', 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this searches through ranked_beers to see how far into the list the beer appears\n",
    "# this was done to test if the return of the state/city search function was returning\n",
    "# a ranked list or not\n",
    "count = 0\n",
    "for i in ranked_beers:\n",
    "    if i[0] == 155828:\n",
    "        print(i)\n",
    "        print(count)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to see how many beers are in a given location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brew_city = breweries[['brewery_id', 'city']]\n",
    "brew_city = brew_city.set_index('brewery_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_explore = pd.merge(beers, brew_city, on='brewery_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_beer = beer_explore.loc[beer_explore.city == 'Los Angeles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_beer = LA_beer.drop(['style', 'availability', 'abv', 'notes'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_reviews = pd.merge(LA_beer, df_with_mins, on = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_reviews.id.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONTENT BASED -- NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_mins.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsets reviews df, and then joins all text reviews for each individual beer together\n",
    "df_joined = df_with_mins.copy()\n",
    "df_joined['joined_text'] = df_joined.groupby('id')['text'].\\\n",
    "                               transform(lambda x: ''.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes duplicate beers, and subsets to just beer id, joined_text and rating info\n",
    "# to be cleaned and then joined to beers df\n",
    "df_joined_sub = df_joined[['id', 'joined_text', 'avg_score', 'no_of_ratings']].drop_duplicates(\\\n",
    "                                                                        subset='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes \\xa0 remove text\n",
    "df_joined_sub['joined_text'] = df_joined_sub['joined_text'].apply(lambda x: re.sub\\\n",
    "                                                                  (r'\\xa0', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like it worked!\n",
    "df_joined_sub.joined_text[0][:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(stop_words='english')\n",
    "counts = count_vect.fit_transform(df_joined_sub.joined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = cosine_similarity(counts, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = pd.Series(df_joined_sub.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices[indices == 4].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendations(beer_id, cos_sim = cos_sim):\n",
    "    \"\"\"\n",
    "    Takes a beer id and cosine similarty matrix in as arguments and returns beers closely related to the input beer\n",
    "    \"\"\"\n",
    "    # initializing the empty list of recommended movies\n",
    "    recommended_beers = []\n",
    "    \n",
    "    # gettin the index of the movie that matches the title\n",
    "    idx = indices[indices == beer_id].index[0]\n",
    "    print(idx)\n",
    "    # creating a Series with the similarity scores in descending order\n",
    "    score_series = pd.Series(cos_sim[idx]).sort_values(ascending = False)\n",
    "\n",
    "    # getting the indexes of the 10 most similar movies\n",
    "    top_10_indexes = list(score_series.iloc[1:11].index)\n",
    "    print(top_10_indexes)\n",
    "    # populating the list with the titles of the best 10 matching movies\n",
    "    for i in top_10_indexes:\n",
    "        recommended_beers.append(list(beers_text.name)[i])\n",
    "        \n",
    "    return recommended_beers\n",
    "\n",
    "def tfidf_recs(beer_id, cos_sim = tfidf_cos):\n",
    "    \"\"\"\n",
    "    Takes a beer id and cosine similarty matrix in as arguments and returns beers closely related to the input beer\n",
    "    \"\"\"\n",
    "    # initializing the empty list of recommended movies\n",
    "    recommended_beers = []\n",
    "    \n",
    "    # gettin the index of the movie that matches the title\n",
    "    idx = indices[indices == beer_id].index[0]\n",
    "\n",
    "    # creating a Series with the similarity scores in descending order\n",
    "    score_series = pd.Series(cos_sim[idx]).sort_values(ascending = False)\n",
    "\n",
    "    # getting the indexes of the 10 most similar movies\n",
    "    top_10_indexes = list(score_series.iloc[1:21].index)\n",
    "    \n",
    "    # populating the list with the titles of the best 10 matching movies\n",
    "    for i in top_10_indexes:\n",
    "        recommended_beers.append(list(beers_text.name)[i])\n",
    "        \n",
    "    return beers_text.name[beer_id], recommended_beers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beers_text = pd.merge(df_joined_sub, beers, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Below is code for 2.0 recommender from Wednesday that might not have saved in JN\n",
    "\n",
    "# how to inpute new ratings\n",
    "user_rating = [{'username': 'tester-mctestyface', 'id': 55245, 'score': 1},\n",
    "               {'username': 'tester-mctestyface', 'id': 237806, 'score': 1},\n",
    "               {'username': 'tester-mctestyface', 'id': 1062, 'score': 1},\n",
    "               {'username': 'tester-mctestyface', 'id': 116702, 'score': 1},\n",
    "               {'username': 'tester-mctestyface', 'id': 140119, 'score': 1},\n",
    "               {'username': 'tester-mctestyface', 'id': 143753, 'score': 1.5},\n",
    "               {'username': 'tester-mctestyface', 'id': 265678, 'score': 1.5},\n",
    "               {'username': 'tester-mctestyface', 'id': 237806, 'score': 1},\n",
    "              ]\n",
    "\n",
    "## add the new ratings to the original ratings DataFrame\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "new_ratings_df = svd_df.append(user_rating,ignore_index=True)\n",
    "new_data = Dataset.load_from_df(new_ratings_df,reader)\n",
    "trainset = new_data.build_full_trainset()\n",
    "model = SVD(n_factors = 20, reg_all=0.02,)\n",
    "model.fit(trainset)\n",
    "\n",
    "# make a set of beers the user did not rate\n",
    "svd_pred_set = get_user_pred_set(user_rating, new_ratings_df)\n",
    "located_beers = svd_location_filter(svd_pred_set, lookup_dict, 'WA', 'Seattle', 500)\n",
    "\n",
    "located_preds = pred_for_user_location(located_beers, 'tester-mctestyface', model)\n",
    "return_top_breweries(located_preds,located_beers,2)\n",
    "\n",
    "def get_user_pred_set(user_rating_list, rating_df):\n",
    "    \"\"\"returns a list of beer id's to be predicted. excludes beers the users imputed\n",
    "    user_rating_list: is a list of dictionaries produced when the user provides\n",
    "                      initial ratings\n",
    "    rating_df: is a df of all ratings subseted to the columns needed for SVD\"\"\"\n",
    "    \n",
    "    user_ratings_ids = []\n",
    "    for rating in user_rating_list:\n",
    "        user_ratings_ids.append(rating['id'])\n",
    "    beers_for_pred = []\n",
    "    for beer_id in rating_df['id']:\n",
    "        if beer_id not in user_ratings_ids:\n",
    "            beers_for_pred.append(str(beer_id))\n",
    "    return set(beers_for_pred)\n",
    "\n",
    "def svd_location_filter(user_pred_list, lookup_dict, state, city, n):\n",
    "    \"\"\" \n",
    "    takes in list from get_user_pred_list and filters list down to only the location they\n",
    "    provided, returns a dictionary with the beer id as key, and beer_name and brewery_name\n",
    "    as values\n",
    "    \"\"\"\n",
    "    located_beer = {}\n",
    "    counter = 0\n",
    "\n",
    "    for beer in user_pred_list:\n",
    "#         print(beer)\n",
    "        if counter < n:\n",
    "            dict_state = lookup_dict[beer]['state']\n",
    "            dict_city = lookup_dict[beer]['city']\n",
    "            brewery_id = lookup_dict[beer]['brewery_id']\n",
    "            brewery_name = lookup_dict[beer]['brewery_name']\n",
    "            beer_name = lookup_dict[beer]['name']\n",
    "            if (dict_state == state) and (dict_city == city):\n",
    "        #             print(beer_breweries_lookup[beer[0]])\n",
    "                if brewery_id in located_beer:\n",
    "                    continue\n",
    "                else:  \n",
    "                    located_beer[beer] = (beer_name,brewery_name)\n",
    "                counter += 1\n",
    "    return located_beer\n",
    "\n",
    "def sort_score(val):\n",
    "    \"\"\"used to sort predictions by their estimated score\"\"\"\n",
    "    return val[1]\n",
    "\n",
    "def pred_for_user_location(to_predict_list, username, model):\n",
    "    \"\"\"Takes in a list of beer ID's (that have been filtered by location) to be predicted for \n",
    "    the given user. Also takes the trained model as an argument\"\"\"\n",
    "    predictions = []\n",
    "    for iid in to_predict_list:\n",
    "        pred = model.predict(username, int(iid), verbose = False)\n",
    "        predictions.append((pred[1],pred[3]))\n",
    "    predictions.sort(key = sort_score, reverse = True)\n",
    "    return predictions\n",
    "\n",
    "def return_top_breweries(top_beers, svd_loc_filter_output, n):\n",
    "    \n",
    "    counter = 0\n",
    "    top_breweries = {}\n",
    "    for beer in top_beers:\n",
    "        if counter < n:\n",
    "            beer_name = svd_loc_filter_output[str(beer[0])][0]\n",
    "            brewery_name = svd_loc_filter_output[str(beer[0])][1]\n",
    "            if brewery_name in top_breweries:\n",
    "                continue\n",
    "            else:\n",
    "                top_breweries[brewery_name] = (brewery_name, beer_name)\n",
    "#             top_breweries.append(svd_loc_filter_output[str(beer[0])])\n",
    "            \n",
    "            \n",
    "            counter+=1\n",
    "    return top_breweries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sklearn-env)",
   "language": "python",
   "name": "sklearn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
